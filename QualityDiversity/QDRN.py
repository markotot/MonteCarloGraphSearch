import randomfrom copy import deepcopyfrom ribs.archives import GridArchivefrom ribs.optimizers import Optimizerfrom ribs.emitters import OptimizingEmitterclass QD_Search:  def __init__(self, width, actions_n, agent_config):    self.all_actions = [i for i in range(actions_n)]    self.width = width    self.num_of_emitters = agent_config['qd_emitters']    self.total_itrs = agent_config['qd_iterations']    self.batch_size = agent_config['qd_batchsize']    self.bounds = None    self.initial_model = None    self.archive = None    self.emitters = None    self.optimizer = None    self.archive_df = None    self.rollout_depth = agent_config['qd_rollout_depth']  def initialize_qd(self):    self.initial_model = [random.choice(self.all_actions) for _ in range(self.rollout_depth)]     self.bounds = [(self.all_actions[0], self.all_actions[-1]) for i in range(len(self.initial_model))]    self.archive = GridArchive([self.width, self.width],                                [(0, self.width), (0, self.width)])    self.emitters = [OptimizingEmitter(self.archive, self.initial_model, 1, batch_size=self.batch_size,                                        bounds = self.bounds) for _ in range(self.num_of_emitters)]     self.optimizer = Optimizer(self.archive, self.emitters)  def rollout(self, env, solution, agent):    total_reward = 0    path = []    actions = [int(a) for a in solution]    simulate_env = deepcopy(env)    previous_observation = simulate_env.get_observation()        for action in actions:        state, r, done, _ = simulate_env.step(action)        agent.forward_model_calls += 1        agent.remaining_budget -= 1        observation = simulate_env.get_observation()        total_reward += r        path.append((previous_observation, observation, action, r, done))        previous_observation = observation        if done:            break    _, _ = agent.add_novelties_to_graph([path])    return path[-1][1], path, total_reward  def get_scores(self, agent, path, total_reward):    score = 0    for p in path:            if agent.graph.has_node(p[1]) == False:        score += 2      else:        node = agent.graph.get_node_info(p[1])        score += node.novelty_value    return score + total_reward*1000  def qd_optimize(self, agent, fake_env):    for itr in range(1, self.total_itrs + 1):      solutions = self.optimizer.ask()      objs, bcs = [], []        for i in range(len(solutions)):        last_step, path, total_reward = self.rollout(fake_env, solutions[i], agent)        diversity_score = self.get_scores(agent, path, total_reward)        objs.append(diversity_score)        bcs.append([last_step[0], last_step[1]])       self.optimizer.tell(objs, bcs)    self.archive_df = self.archive.as_pandas().astype(int)  def get_qd_elite(self, agent, fake_env):    solutions = []    sol_cols = ['solution_'+str(i) for i in range(self.archive_df.shape[1]-5)]    for i in self.archive_df.loc[self.archive_df.objective==self.archive_df.objective.max()].index:      solutions.append(self.archive_df.loc[i, sol_cols].values)    actions = random.choice(solutions)    return actions   def qd_search(self, agent, fake_env):    self.initialize_qd()    self.qd_optimize(agent, fake_env)    actions = self.get_qd_elite(agent, fake_env)    return actions